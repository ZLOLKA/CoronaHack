{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jaroslav\\Desktop\\Job\\Python\\DS\\CoronaHack\\venv\\Scripts\n",
      "C:\\Python\\Python38\\Scripts\\\n",
      "C:\\Python\\Python38\\\n",
      "C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.2\\bin\n",
      "C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.2\\libnvvp\n",
      "C:\\Python\\Python39\\Scripts\\\n",
      "C:\\Python\\Python39\\\n",
      "C:\\WINDOWS\\system32\n",
      "C:\\WINDOWS\n",
      "C:\\WINDOWS\\System32\\Wbem\n",
      "C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\\n",
      "C:\\WINDOWS\\System32\\OpenSSH\\\n",
      "C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common\n",
      "C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR\n",
      "C:\\Go\\bin\n",
      "C:\\Program Files\\Git\\cmd\n",
      "C:\\Program Files\\Perforce\\\n",
      "C:\\WINDOWS\\system32\n",
      "C:\\WINDOWS\n",
      "C:\\WINDOWS\\System32\\Wbem\n",
      "C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\\n",
      "C:\\WINDOWS\\System32\\OpenSSH\\\n",
      "C:\\Program Files\\NVIDIA Corporation\\Nsight Compute 2020.3.1\\\n",
      "C:\\Users\\Jaroslav\\Python\\Python38\\Scripts\\\n",
      "C:\\Users\\Jaroslav\\Python\\Python38\n",
      "C:\\Users\\Jaroslav\\Desktop\\Job\\Bazel\n",
      "C:\\Users\\Jaroslav\\AppData\\Local\\Microsoft\\WindowsApps\n"
     ]
    }
   ],
   "source": [
    "# INFO \n",
    "# https://coderoad.ru/44853059/Tensorflow-%D1%81%D0%BE%D0%BE%D0%B1%D1%89%D0%B5%D0%BD%D0%B8%D1%8F-%D0%B6%D1%83%D1%80%D0%BD%D0%B0%D0%BB%D0%B0-%D0%BD%D0%B5-%D0%BE%D1%82%D0%BE%D0%B1%D1%80%D0%B0%D0%B6%D0%B0%D1%8E%D1%82%D1%81%D1%8F\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'\n",
    "os.environ['TF_CPP_MIN_VLOG_LEVEL'] = '0'\n",
    "[print(x) for x in os.environ[\"PATH\"].split(\";\")];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time import Numpy = Wall time: 312 ms\n",
      "Time import Pandas = Wall time: 478 ms\n",
      "Time import matplotlib.pyplot = Wall time: 520 ms\n",
      "Time import seaborn = Wall time: 709 ms\n",
      "Time import tqdm = Wall time: 11 ms\n",
      "Time import tensorflow = Wall time: 3.77 s\n",
      "Time import tensorflow.keras = Wall time: 0 ns\n",
      "Time import tensorflow.keras.layers = Wall time: 0 ns\n",
      "Wall time: 5.92 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#print(\"Time import CV2 = \", end='')\n",
    "#%time import cv2\n",
    "print(\"Time import Numpy = \", end='')\n",
    "%time import numpy as np\n",
    "print(\"Time import Pandas = \", end='')\n",
    "%time import pandas as pd\n",
    "print(\"Time import matplotlib.pyplot = \", end='')\n",
    "%time from matplotlib import pyplot as plt\n",
    "print(\"Time import seaborn = \", end='')\n",
    "%time import seaborn as sns\n",
    "print(\"Time import tqdm = \", end='')\n",
    "%time from tqdm import tqdm\n",
    "#print(\"Time import sklearn.decomposition.PCA = \", end='')\n",
    "#%time from sklearn.decomposition import PCA\n",
    "\n",
    "#For normal work ipywidgets\n",
    "#!jupyter labextension install @jupyter-widgets/jupyterlab-manager\n",
    "#print(\"Time import ipywidgets.interact and ipywidgets.interact_manual = \", end='')\n",
    "#%time from ipywidgets import interact, interact_manual\n",
    "\n",
    "print(\"Time import tensorflow = \", end='')\n",
    "%time import tensorflow as tf\n",
    "print(\"Time import tensorflow.keras = \", end='')\n",
    "%time import tensorflow.keras as keras\n",
    "print(\"Time import tensorflow.keras.layers = \", end='')\n",
    "%time from tensorflow.keras import layers\n",
    "\n",
    "# For work using XLA\n",
    "tf.config.optimizer.set_jit(True)\n",
    "\n",
    "# For graphs display in JupyterNotebook\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # For disable SettingWithCopyWarning, arising from the ambiguity of change\n",
    "path = %pwd\n",
    "train_path = path + \"\\\\Coronahack-Chest-XRay-Dataset\\\\Coronahack-Chest-XRay-Dataset\\\\train\\\\\"\n",
    "test_path = path + \"\\\\Coronahack-Chest-XRay-Dataset\\\\Coronahack-Chest-XRay-Dataset\\\\test\\\\\"\n",
    "\n",
    "NUM_DISEASE_TYPE = 7\n",
    "\n",
    "# If (fast == True) used precompile variable mean image size; else this variable is calculated now\n",
    "fast = True\n",
    "\n",
    "# TODO:\n",
    "#  Add the ability to specify a percentage of the RAM of the device on which notepad is running\n",
    "#  For example:\n",
    "#   There are only 8 GB of RAM on the machine, we indicate the number 50 corresponding to 50%,\n",
    "#   then 4 will be entered into the variable; and if 32 GB and 25% then the variable will be 8\n",
    "\n",
    "num_GB = 5 # Number of gigabytes of RAM allocated for storing images data\n",
    "img_type = np.float64 # The type in which the image data will be stored\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and pretreatment data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(path + '\\\\Chest_xray_Corona_Metadata.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(\"Unnamed: 0\", axis=1, inplace=True) # Delete \"Unnamed: 0\" column because it enumerating objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5910 entries, 0 to 5909\n",
      "Data columns (total 5 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   X_ray_image_name        5910 non-null   object\n",
      " 1   Label                   5910 non-null   object\n",
      " 2   Dataset_type            5910 non-null   object\n",
      " 3   Label_2_Virus_category  69 non-null     object\n",
      " 4   Label_1_Virus_category  4334 non-null   object\n",
      "dtypes: object(5)\n",
      "memory usage: 231.0+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Normal', 'Pnemonia'}\n",
      "{nan, 'Stress-Smoking', 'bacteria', 'Virus'}\n",
      "{nan, 'COVID-19', 'SARS', 'Streptococcus', 'ARDS'}\n"
     ]
    }
   ],
   "source": [
    "print(set(data[\"Label\"]))\n",
    "print(set(data[\"Label_1_Virus_category\"]))\n",
    "print(set(data[\"Label_2_Virus_category\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(df: pd.DataFrame) -> pd.Series:\n",
    "    res = pd.Series(range(df.shape[0]), dtype=np.uint8)\n",
    "    for i in range(df.shape[0]):\n",
    "        if df[\"Label\"][i] == \"Normal\":\n",
    "            res[i] = 0 # \"Normal\"\n",
    "            continue\n",
    "        if df[\"Label_1_Virus_category\"][i] == \"Stress-Smoking\":\n",
    "            res[i] = 1 # \"Stress-Smoking\"\n",
    "            continue\n",
    "        if df[\"Label_1_Virus_category\"][i] == \"Virus\":\n",
    "            if df[\"Label_2_Virus_category\"][i] == \"COVID-19\":\n",
    "                res[i] = 2 # \"COVID-19\"\n",
    "                continue\n",
    "            if df[\"Label_2_Virus_category\"][i] == \"SARS\":\n",
    "                res[i] = 3 # \"SARS\"\n",
    "                continue\n",
    "            res[i] = 4 # \"Undefined Virus\"\n",
    "            continue\n",
    "        if df[\"Label_1_Virus_category\"][i] == \"bacteria\":\n",
    "            if df[\"Label_2_Virus_category\"][i] == \"Streptococcus\":\n",
    "                res[i] = 5 # \"Streptococcus\"\n",
    "                continue\n",
    "            res[i] = 6 # \"Undefined Bacteria\"\n",
    "            continue\n",
    "    return res.astype(\"category\")\n",
    "data = data.assign(Disease_type=gen)\n",
    "data = data.drop([\"Label\", \"Label_1_Virus_category\", \"Label_2_Virus_category\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = data[data[\"Dataset_type\"] == \"TRAIN\"].drop([\"Dataset_type\"], axis=1)\n",
    "test = data[data[\"Dataset_type\"] == \"TEST\"].drop([\"Dataset_type\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min image size = (127, 255)\n",
      "Average image size = (1008, 1354)\n",
      "Max image size = (3480, 4248)\n",
      "Old time if not fast = 1min 30s\n",
      "New time if not fast use np = 1min 12s\n",
      "Wall time: 3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if not fast:\n",
    "    shapes = set()\n",
    "    train_df[\"X_ray_image_name\"].map(\n",
    "        lambda img: shapes.add(cv2.imread(train_path + img, cv2.IMREAD_GRAYSCALE).shape)\n",
    "    )\n",
    "    shapes = np.array(list(shapes))\n",
    "    \n",
    "    min_img_size  = tuple(map(lambda x: int(x.min()),  shapes.T))\n",
    "    mean_img_size = tuple(map(lambda x: int(x.mean()), shapes.T))\n",
    "    max_img_size  = tuple(map(lambda x: int(x.max()),  shapes.T))\n",
    "else:\n",
    "    min_img_size = (127, 255)\n",
    "    mean_img_size = (1008, 1354)\n",
    "    max_img_size = (3480, 4248)\n",
    "print(f\"Min image size = {min_img_size}\")\n",
    "print(f\"Average image size = {mean_img_size}\")\n",
    "print(f\"Max image size = {max_img_size}\")\n",
    "print(\"Old time if not fast = 1min 30s\")\n",
    "print(\"New time if not fast use np = 1min 12s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average image size = (1008, 1354)\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if not fast:\n",
    "    img_sizes = set()\n",
    "    for img in tqdm(train_df[\"X_ray_image_name\"]):\n",
    "        img_d = cv2.imread(train_path + img, cv2.IMREAD_GRAYSCALE)\n",
    "        img_sizes.add(img_d.shape)\n",
    "        \n",
    "    sum_img_sizes = [0, 0]\n",
    "    for i, j in img_sizes:\n",
    "        sum_img_sizes[0] += i\n",
    "        sum_img_sizes[1] += j\n",
    "    mean_img_size = tuple(int(i / len(img_sizes)) for i in sum_img_sizes)\n",
    "else:\n",
    "    mean_img_size = (1008, 1354) # Counted before\n",
    "print(f\"Average image size = {mean_img_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_df.sample(frac=0.7, random_state=1).reset_index(drop=True)\n",
    "valid = train_df.drop(train.index).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgGen:\n",
    "    _first = True\n",
    "    \n",
    "    def __new__(cls, df, *args, reset=False, epochs=10, batch_size=16, **kwargs):\n",
    "        cls._data = df\n",
    "        if reset:\n",
    "            cls._first = reset\n",
    "        if cls._first:\n",
    "            print(1)\n",
    "            data = iter(\n",
    "                tf.data.Dataset.range(epochs)\n",
    "                .interleave(\n",
    "                    cls._dataset_gen,\n",
    "                    num_parallel_calls=tf.data.AUTOTUNE,\n",
    "                )\n",
    "                .map(\n",
    "                    cls._read,\n",
    "                    num_parallel_calls=tf.data.AUTOTUNE\n",
    "                )\n",
    "                .map(\n",
    "                    cls._resize,\n",
    "                    num_parallel_calls=tf.data.AUTOTUNE\n",
    "                )\n",
    "                .batch(batch_size)\n",
    "            )\n",
    "            cls._first = False\n",
    "            cls.__obj = data\n",
    "            return data#[next(data) for _ in range(batch_size)]\n",
    "        else:\n",
    "            print(9)\n",
    "            return cls.__obj\n",
    "            \n",
    "        \n",
    "    @classmethod\n",
    "    def _dataset_gen(cls, *args):\n",
    "        return tf.data.Dataset.from_generator(\n",
    "            cls._gen,\n",
    "            output_signature=(\n",
    "                tf.TensorSpec(\n",
    "                    shape=(),\n",
    "                    dtype=tf.string\n",
    "                ),\n",
    "                tf.TensorSpec(\n",
    "                    shape=(7, ),\n",
    "                    dtype=tf.float16\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def _gen(cls):\n",
    "        for img_name, label in cls._data[:2].values:\n",
    "            label_tf_categ = np.zeros(NUM_DISEASE_TYPE, dtype=np.float16) # Label in tf categorical type\n",
    "            label_tf_categ[label] = 1.\n",
    "            yield tf.constant(train_path + img_name), tf.constant(label_tf_categ)\n",
    "\n",
    "    @classmethod\n",
    "    def _read(cls, img_path: tf.string, label: tf.int8):\n",
    "        image = tf.io.read_file(img_path)\n",
    "        image = tf.image.decode_jpeg(image, channels=1, )\n",
    "        return image, label\n",
    "\n",
    "    @classmethod\n",
    "    def _resize(cls, x, label: tf.int8):\n",
    "        return tf.image.resize(x, max_img_size, method=\"area\"), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 3478, 4246, 32)    320       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 3478, 4246, 32)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 1739, 2123, 32)    0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 1737, 2121, 32)    9248      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1737, 2121, 32)    0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 1735, 2119, 32)    9248      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1735, 2119, 32)    0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 1733, 2117, 32)    9248      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1733, 2117, 32)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 866, 1058, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 864, 1056, 32)     9248      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 864, 1056, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 862, 1054, 32)     9248      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 862, 1054, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 860, 1052, 32)     9248      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 860, 1052, 32)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 430, 526, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 428, 524, 32)      9248      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 428, 524, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 426, 522, 32)      9248      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 426, 522, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 424, 520, 32)      9248      \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 424, 520, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 212, 260, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 210, 258, 32)      9248      \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 210, 258, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 208, 256, 32)      9248      \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 208, 256, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 206, 254, 32)      9248      \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 206, 254, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 103, 127, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 101, 125, 32)      9248      \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 101, 125, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 99, 123, 32)       9248      \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 99, 123, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 97, 121, 32)       9248      \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 97, 121, 32)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 48, 60, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 92160)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 7)                 645127    \n",
      "=================================================================\n",
      "Total params: 784,167\n",
      "Trainable params: 784,167\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(*max_img_size, 1)),\n",
    "    layers.Dropout(.3),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "    layers.Dropout(.3),\n",
    "    layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "    layers.Dropout(.3),\n",
    "    layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "    layers.Dropout(.3),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "    layers.Dropout(.3),\n",
    "    layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "    layers.Dropout(.3),\n",
    "    layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "    layers.Dropout(.3),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "    layers.Dropout(.3),\n",
    "    layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "    layers.Dropout(.3),\n",
    "    layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "    layers.Dropout(.3),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "    layers.Dropout(.3),\n",
    "    layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "    layers.Dropout(.3),\n",
    "    layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "    layers.Dropout(.3),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "    layers.Dropout(.3),\n",
    "    layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "    layers.Dropout(.3),\n",
    "    layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "    layers.Dropout(.3),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.Flatten(),\n",
    "    #layers.Dense(64, activation=\"relu\"),\n",
    "    #layers.Dropout(.3),\n",
    "    #layers.Dense(32, activation=\"relu\"),\n",
    "    #layers.Dropout(.3),\n",
    "    #layers.Dense(64, activation=\"relu\"),\n",
    "    #layers.Dropout(.3),\n",
    "    layers.Dense(7, activation=\"sigmoid\")\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 18 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Epoch 1/2\n"
     ]
    }
   ],
   "source": [
    "train_gen = ImgGen(train, reset=True, batch_size=1, epochs=2)\n",
    "#valid_gen=\n",
    "\n",
    "model.fit(\n",
    "    train_gen,\n",
    "    #validation_data=valid_gen,\n",
    "    epochs=2,\n",
    "    #use_multiprocessing=True,\n",
    "    verbose=1,\n",
    "    workers=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Warning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgGen(keras.utils.Sequence):\n",
    "    def __init__(self, df, batch_size=32, dim=(32,32,32), n_channels=1,\n",
    "                 n_classes=10, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.data = df[\"X_ray_image_name\"].T\n",
    "        self.labels = df[\"Disease_type\"]\n",
    "        self.batch_size = batch_size\n",
    "        self.dim = dim\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.data) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size : (index+1)*self.batch_size]\n",
    "\n",
    "        # Find Data Frame\n",
    "        df_temp = [self.data[k] for k in indexes]\n",
    "        \n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(df_temp)\n",
    "\n",
    "        return X, y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.data))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            \n",
    "    def __data_generation(self, df_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        \n",
    "        size = self.batch_size\n",
    "        \n",
    "        \n",
    "        # Initialization\n",
    "        X = np.empty((size, *self.dim, self.n_channels), dtype=np.uint8)\n",
    "        y = np.empty(size, dtype=np.uint8)\n",
    "        \n",
    "        # Generate data\n",
    "        for i, ID in enumerate(df_temp):\n",
    "            # Store sample         \n",
    "            img = cv2.imread(train_path + ID, cv2.IMREAD_GRAYSCALE)\n",
    "            rsz_img = cv2.resize(img, (mean_img_size[1], mean_img_size[0]), interpolation=cv2.INTER_AREA)\n",
    "            X[i,] = (rsz_img / 255).reshape((*rsz_img.shape, 1))\n",
    "\n",
    "            # Store class\n",
    "            y[i] = self.labels[i]\n",
    "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "params_for_ImgGen = {\n",
    "    'dim': mean_img_size,\n",
    "    'batch_size': 8,\n",
    "    'n_classes': 7,\n",
    "    'n_channels': 1,\n",
    "    'shuffle': True,\n",
    "}\n",
    "\n",
    "training_generator = ImgGen(train, **params_for_ImgGen)\n",
    "validation_generator = ImgGen(valid, **params_for_ImgGen)\n",
    "\n",
    "n = train.shape[0]\n",
    "n0, n1, n2, n3, n4, n5, n6 = 0, 0, 0, 0, 0, 0, 0\n",
    "for i in train[\"Disease_type\"]:\n",
    "    if i == 0:\n",
    "        n0 += 1\n",
    "    elif i == 1:\n",
    "        n1 += 1\n",
    "    elif i == 2:\n",
    "        n2 += 1\n",
    "    elif i == 3:\n",
    "        n3 += 1\n",
    "    elif i == 4:\n",
    "        n4 += 1\n",
    "    elif i == 5:\n",
    "        n5 += 1\n",
    "    else:\n",
    "        n6 += 1\n",
    "class_weight={\n",
    "    0: 1-n0/n,\n",
    "    1: 1-n1/n,\n",
    "    2: 1-n2/n,\n",
    "    3: 1-n3/n,\n",
    "    4: 1-n4/n,\n",
    "    5: 1-n5/n,\n",
    "    6: 1-n6/n,\n",
    "}\n",
    "\n",
    "model.fit(\n",
    "    training_generator,\n",
    "    validation_data=validation_generator,\n",
    "    class_weight=class_weight,\n",
    "    epochs=10,\n",
    "    verbose=1,\n",
    "    use_multiprocessing=True,\n",
    "    workers=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Warning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"my_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = mean_img_size[0] * mean_img_size[1]\n",
    "x = int(num_GB * (1024 ** 3) / (img_type().itemsize * y)) # (1024 ** 3) - number byte in gigabyte\n",
    "img_data = np.zeros((x, y), dtype=img_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, img_name in tqdm(enumerate(train[\"X_ray_image_name\"][:x])):\n",
    "    img = cv2.imread(train_path + img_name, cv2.IMREAD_GRAYSCALE)\n",
    "    rsz_img = cv2.resize(img, mean_img_size, interpolation=cv2.INTER_AREA)\n",
    "    img_d = rsz_img.ravel() / 255\n",
    "    img_data[i] = img_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_img_d = pd.DataFrame(img_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data_T = img_data.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact_manual\n",
    "def _(x=\"\", y=\"\"):\n",
    "    sns.distplot(pd_img_d[:int(x)][:int(y)])\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(pd_img_d[:3][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pca = PCA(n_components=2)\n",
    "pca_t = pca.fit_transform(pd_img_d.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./pca_t1.npy\", pca_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_t = np.load(\"./pca_t.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pca_1 = PCA(n_components=1)\n",
    "pca_t_1 = pca_1.fit_transform(pca_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_t_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_t_1 = pca_t_1.T * 255\n",
    "plt.imshow(pca_t_1.reshape(mean_img_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sns.heatmap(pd_img_d.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"unic CoD = \", unic_CoD, \"\\n\")\n",
    "print(\"unic ToD = \", unic_ToD, \"\\n\")\n",
    "@interact\n",
    "def test(x=data.columns[1:], y=data.columns[1:]):\n",
    "    return pd.crosstab(data[x], data[y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"Cause_of_disease\"].unique() # => \"Cause_of_disease\" target feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All information on program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
